{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/csh/.virtualenvs/tf-gpu/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "% pylab inline\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction = 0.5, allow_growth=True)\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "set_session(tf.Session(config = tf.ConfigProto(gpu_options = gpu_options)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "import os\n",
    "path.insert(0, \"/home/csh/dev/evaxml/\")\n",
    "\n",
    "from preprocessing.encoders import SequenceIntEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequences(filename):\n",
    "    from Bio import SeqIO\n",
    "    proteins = {r.id: r.seq for r in SeqIO.parse(filename, 'fasta')}\n",
    "    return proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(filename):\n",
    "    \"\"\" Load features into dataframes \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partitions(filename):\n",
    "    import pandas as pd\n",
    "    return pd.read_csv(filename, \n",
    "                       sep='\\t', \n",
    "                       header=None, \n",
    "                       index_col=None, \n",
    "                       names=[\"ProteinID\", \"Partition\", \"Weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_epoch_size(weights):\n",
    "    import numpy as np\n",
    "    weights = weights / np.max( weights )\n",
    "    return int(np.ceil(np.sum(weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_indices(indices, weights, batch_size=64, seed=1, max_epoch_size=None):\n",
    "    \"\"\"\n",
    "    indices          np.array (N)    List of data indices to sample from\n",
    "    weights          np.array (N)    List of data weights to use for sampling probability\n",
    "    batch_size       int             Number of proteins to sample from positive and negative\n",
    "    \n",
    "    returns (yield)  np.array (M)    Yields lists of indices (M=batch_size) indefinitely.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    randgen = np.random.RandomState(seed)\n",
    "    \n",
    "    weights = weights / np.max( weights )\n",
    "    probabilities = weights / np.sum( weights )\n",
    "    epoch_size = calc_epoch_size(weights)\n",
    "    assert batch_size < epoch_size\n",
    "    \n",
    "    while True:\n",
    "        epoch_indices = randgen.choice(indices, size=epoch_size, replace=False, p=probabilities)\n",
    "        for i in range(0, len(epoch_indices), batch_size):\n",
    "            if len(epoch_indices[i:i + batch_size]) == batch_size:\n",
    "                yield epoch_indices[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(partitions_pos, partitions_neg, batch_size, target_values, sequences, blosum_matrix, seed=1, padding_length=None, max_epoch_size=None):\n",
    "    from itertools import izip\n",
    "    import random\n",
    "    # Decide on maximum length\n",
    "    if padding_length == None:\n",
    "        padding_length = max( [len(s) for s in sequences.values()] )\n",
    "    else:\n",
    "        assert type(padding_length) == int\n",
    "        \n",
    "    for batch_pos, batch_neg in izip( generate_batch_indices(partitions_pos.ProteinID, partitions_pos.Weight, batch_size/2),\n",
    "                                     generate_batch_indices(partitions_neg.ProteinID, partitions_neg.Weight, batch_size/2)):\n",
    "        batch = list(batch_pos) + list(batch_neg)\n",
    "        random.shuffle(batch) # inplace\n",
    "        \n",
    "        batch_sequences = sequences_int.loc[ batch ]#.as_matrix()\n",
    "        batch_target_values = target_values.loc[batch]#.as_matrix()\n",
    "        \n",
    "        yield batch_sequences, batch_target_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_soluble = load_partitions(\"../data/partitioning/targettrack.soluble.partitions.csv\")\n",
    "partition_insoluble = load_partitions(\"../data/partitioning/targettrack.insoluble.partitions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_soluble = load_sequences(\"../data/raw/targettrack.soluble.fasta\")\n",
    "sequences_insoluble = load_sequences(\"../data/raw/targettrack.insoluble.fasta\")\n",
    "sequences = dict(sequences_soluble, **sequences_insoluble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_int = SequenceIntEncoder(sequences, padding_length=1000, ignore=\"Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_values = pd.concat([\n",
    "    pd.Series(index=sequences_soluble.keys(), data=1),\n",
    "    pd.Series(index=sequences_insoluble.keys(), data=0)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_batch(partitions_pos=partition_soluble, \n",
    "               partitions_neg=partition_insoluble, \n",
    "               batch_size=30, \n",
    "               target_values=target_values, \n",
    "               sequences=sequences, \n",
    "               blosum_matrix=None,\n",
    "               padding_length=1000).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input\n",
    "from keras.layers import merge\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Masking\n",
    "from keras.layers import Merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D, MaxPooling2D, AveragePooling1D, AveragePooling2D\n",
    "from keras.layers.pooling import GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.optimizers import Adam, SGD, Adagrad, RMSprop\n",
    "from keras.initializers import Ones, Zeros, Constant, RandomUniform, RandomNormal, glorot_normal, glorot_uniform\n",
    "from keras import backend as K\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers.core import RepeatVector\n",
    "from keras.layers.core import Permute\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.merge import Multiply\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras.constraints import max_norm, unit_norm, non_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add custom metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_keras_metric(method):\n",
    "    import functools\n",
    "    from keras import backend as K\n",
    "    import tensorflow as tf\n",
    "    @functools.wraps(method)\n",
    "    def wrapper(self, args, **kwargs):\n",
    "        \"\"\"Wrapper for turning tensorflow metrics into keras metrics \n",
    "        \n",
    "        Usage: \n",
    "        See https://stackoverflow.com/questions/45947351/how-to-use-tensorflow-metrics-in-keras\n",
    "        \"\"\"\n",
    "        value, update_op = method(self, args, **kwargs)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([update_op]):\n",
    "            value = tf.identity(value)\n",
    "        return value\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_roc = as_keras_metric(tf.metrics.auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse =  as_keras_metric(tf.metrics.mean_squared_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@as_keras_metric\n",
    "def auc_pr(y_true, y_pred, curve='PR'):\n",
    "    return tf.metrics.auc(y_true, y_pred, curve=curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(input_shape=(750, 20),\n",
    "                          # convolutions\n",
    "                          conv_filters=2, \n",
    "                          conv_filter_lengths=[2, 2], \n",
    "                          conv_strides=[1, 1],\n",
    "                          conv_padding=\"same\",\n",
    "                          conv_pooling=\"average\",\n",
    "                          conv_pool_length=3,\n",
    "                          conv_concat_axis=1,\n",
    "                  \n",
    "                          # recurrent\n",
    "                          lstm=None,\n",
    "                          lstm_bidirectional=True,\n",
    "                          lstm_hidden=3,\n",
    "                          lstm_drop_input=0,\n",
    "                          lstm_drop_recurrent=0,\n",
    "                          lstm_return_sequences=False,\n",
    "                  \n",
    "                          # dense\n",
    "                          dense_hidden=4,\n",
    "                          final_activation=\"sigmoid\",\n",
    "                          \n",
    "                          # overall\n",
    "                          bias_init=\"zeros\",\n",
    "                          dropout_probability=0.0,\n",
    "                          loss=\"binary_crossentropy\", # 'binary_crossentropy'\n",
    "                          optimizer=\"rmsprop\",\n",
    "                          clipvalue=None,\n",
    "                          clipnorm=None,\n",
    "                          learning_rate=0.001,\n",
    "                          decay=None,\n",
    "                  \n",
    "                          # metrics\n",
    "                          metrics=['accuracy']\n",
    "                         ):\n",
    "    \n",
    "    assert type(input_shape)==tuple,\"Input shape must be a tuple\"\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ######################################### Define multi-input #########################################\n",
    "    ######################################################################################################\n",
    "    # Sequence input\n",
    "    l_input = Input(shape=input_shape, name='sequence-input')\n",
    "    l_embed = Embedding(input_dim=21,\n",
    "                        output_dim=20,\n",
    "                        input_length=input_shape[0],\n",
    "                        mask_zero=False,\n",
    "                        )(l_input)\n",
    "\n",
    "    ######################################################################################################\n",
    "    ######################################### Convolutions ###########################################\n",
    "    ######################################################################################################\n",
    "    # sequence convolutions\n",
    "    conv_layers = []\n",
    "    for i in range(len(conv_filter_lengths)):\n",
    "        c1_layer = Conv1D(filters=conv_filters, \n",
    "                         kernel_size=conv_filter_lengths[i], \n",
    "                         strides=conv_strides[i],\n",
    "                         padding=conv_padding,\n",
    "                         name='conv-seq-%d' % i)(l_embed)\n",
    "        \n",
    "        if conv_pooling == 'max':\n",
    "            c1_layer = MaxPooling1D(pool_size=conv_pool_length, name='max-seq-%d' % i)(c1_layer)\n",
    "        elif conv_pooling == 'average':\n",
    "            c1_layer = AveragePooling1D(pool_size=conv_pool_length, name='av-seq-%d' % i)(c1_layer)\n",
    "        elif conv_pooling == 'average_max':\n",
    "            conv_pool_layers = []\n",
    "            c1_layer1 = MaxPooling1D(pool_size=conv_pool_length)(c1_layer)\n",
    "            conv_pool_layers.append(c1_layer1)\n",
    "            c1_layer2 = AveragePooling1D(pool_size=conv_pool_length)(c1_layer)\n",
    "            conv_pool_layers.append(c1_layer2)\n",
    "            c1_layer = Concatenate(axis=conv_concat_axis, name='av-max-seq-%d' % i)(conv_pool_layers)\n",
    "        conv_layers.append( c1_layer )\n",
    "        \n",
    "    # Merging all filters into one set\n",
    "    l_main = Concatenate(axis=conv_concat_axis, name='merge-conv')(conv_layers)\n",
    "    ######################################################################################################\n",
    "    ######################################### Normalization ##############################################\n",
    "    ######################################################################################################\n",
    "    l_main = BatchNormalization(name=\"normalization\")(l_main)\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ######################################### Dropouts ###################################################\n",
    "    ######################################################################################################\n",
    "    l_main = Dropout(dropout_probability)(l_main)\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ######################################### Recurrents #################################################\n",
    "    ######################################################################################################\n",
    "    if lstm:\n",
    "        if not lstm_bidirectional:\n",
    "            l_main = LSTM(lstm_hidden,\n",
    "                              dropout=rnn_drop_input,\n",
    "                              recurrent_dropout=lstm_drop_recurrent,\n",
    "                              return_sequences=lstm_return_sequences, name='uni-LSTM')(l_main)\n",
    "        else:\n",
    "            l_main = Bidirectional(LSTM(lstm_hidden,\n",
    "                                          dropout=lstm_drop_input,\n",
    "                                          recurrent_dropout=lstm_drop_recurrent,\n",
    "                                          return_sequences=lstm_return_sequences), name='bi-LSTM')(l_main)\n",
    "    \n",
    "    if lstm_return_sequences==True:\n",
    "        l_main = Flatten()(l_main)\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ######################################### Dense learning #############################################\n",
    "    ######################################################################################################\n",
    "    l_main = Dense(dense_hidden)(l_main)\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ######################################### Dense outputs ##############################################\n",
    "    ######################################################################################################\n",
    "    l_output = Dense(1, activation=final_activation, name='output')(l_main)\n",
    "    \n",
    "    ######################################################################################################\n",
    "    ######################################### Wrap model #################################################\n",
    "    ######################################################################################################\n",
    "    model = Model(outputs=[l_output], inputs=[l_input])\n",
    "    \n",
    "    # Prepare optimizer\n",
    "    #optimizer_opts = dict(clipvalue=clipvalue)\n",
    "    optimizer_opts = dict()\n",
    "    if learning_rate:\n",
    "        optimizer_opts.update(lr=learning_rate)\n",
    "    if decay:\n",
    "        optimizer_opts.update(decay=decay)\n",
    "    if clipvalue:\n",
    "        optimizer_opts.update(clipvalue=clipvalue)\n",
    "    if clipnorm:\n",
    "        optimizer_opts.update(clipnorm=clipnorm)\n",
    "        \n",
    "    if optimizer.lower()=='adam':\n",
    "        optimizer = Adam(**optimizer_opts)\n",
    "    elif optimizer.lower()=='adagrad':\n",
    "        optimizer = Adagrad(**optimizer_opts)\n",
    "    elif optimizer.lower()=='sgd':\n",
    "        optimizer = SGD(**optimizer_opts)\n",
    "    elif optimizer.lower()=='rmsprop':\n",
    "        optimizer = RMSprop(**optimizer_opts)\n",
    "    elif optimizer.lower()=='amsgrad':\n",
    "        optimizer_opts.update(amsgrad=True)\n",
    "        optimizer = Adam(**optimizer_opts)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown optimizer '%s'. Valid options are 'adam', 'adagrad' and 'sgd'\" % optimizer)\n",
    "        \n",
    "    # Compile model\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    # Enable predict_proba method for AUC scoring\n",
    "    model.predict_proba = model.predict    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    auc_roc = as_keras_metric(tf.metrics.auc)\n",
    "    return compile_model(input_shape=(1000,),\n",
    "                                      # convolutions\n",
    "                                      conv_filters=3, \n",
    "                                      conv_filter_lengths=[2, 8, 16], \n",
    "                                      conv_strides=[2, 2, 2],\n",
    "                                      conv_padding=\"same\",\n",
    "                                      conv_pooling=\"average\",\n",
    "                                      conv_pool_length=10,\n",
    "                                      conv_concat_axis=2,\n",
    "\n",
    "                                      # recurrent\n",
    "                                      lstm=True,\n",
    "                                      lstm_bidirectional=True,\n",
    "                                      lstm_hidden=5,\n",
    "                                      lstm_drop_input=0,\n",
    "                                      lstm_drop_recurrent=0,\n",
    "                                      lstm_return_sequences=True,\n",
    "\n",
    "                                      # dense\n",
    "                                      dense_hidden=16,\n",
    "                                      final_activation=\"sigmoid\",\n",
    "\n",
    "                                      # overall\n",
    "                                      bias_init=\"zeros\",\n",
    "                                      dropout_probability=0.0,\n",
    "                                      loss=\"binary_crossentropy\", # 'binary_crossentropy'\n",
    "                                      optimizer=\"rmsprop\",\n",
    "                                      clipvalue=0.5,\n",
    "                                      clipnorm=None,\n",
    "                                      learning_rate=0.005,\n",
    "                                      decay=None,\n",
    "                               \n",
    "                                      #metrics\n",
    "                                      metrics=[auc_roc, mse])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/csh/.virtualenvs/tf-gpu/local/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequence-input (InputLayer)     (None, 1000)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1000, 20)     420         sequence-input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv-seq-0 (Conv1D)             (None, 500, 3)       123         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv-seq-1 (Conv1D)             (None, 500, 3)       483         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv-seq-2 (Conv1D)             (None, 500, 3)       963         embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "av-seq-0 (AveragePooling1D)     (None, 50, 3)        0           conv-seq-0[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "av-seq-1 (AveragePooling1D)     (None, 50, 3)        0           conv-seq-1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "av-seq-2 (AveragePooling1D)     (None, 50, 3)        0           conv-seq-2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "merge-conv (Concatenate)        (None, 50, 9)        0           av-seq-0[0][0]                   \n",
      "                                                                 av-seq-1[0][0]                   \n",
      "                                                                 av-seq-2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "normalization (BatchNormalizati (None, 50, 9)        36          merge-conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50, 9)        0           normalization[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bi-LSTM (Bidirectional)         (None, 50, 10)       600         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 500)          0           bi-LSTM[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           8016        flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            17          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 10,658\n",
      "Trainable params: 10,640\n",
      "Non-trainable params: 18\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_split(parts, n_test=1):\n",
    "    \"\"\" Return combinations (train,test) of partition indices for cross validation \"\"\"\n",
    "    from itertools import combinations\n",
    "    if type(parts) == int:\n",
    "        parts = range(parts)\n",
    "    for test in combinations( parts, n_test ):\n",
    "        yield [x for x in parts if x not in test], list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 0 on training partitions [1, 2, 3] and test partitions [0]\n",
      "   Training data: 29607\n",
      "   Testing data: 17650\n",
      "Epoch 1/500\n",
      "42/42 [==============================] - 15s 350ms/step - loss: 0.9072 - auc: 0.4983 - mean_squared_error: 0.3216 - val_loss: 0.7713 - val_auc: 0.5478 - val_mean_squared_error: 0.2786\n",
      "Epoch 2/500\n",
      "42/42 [==============================] - 13s 309ms/step - loss: 0.6473 - auc: 0.5353 - mean_squared_error: 0.2809 - val_loss: 0.6385 - val_auc: 0.5817 - val_mean_squared_error: 0.2620\n",
      "Epoch 3/500\n",
      "42/42 [==============================] - 13s 310ms/step - loss: 0.6065 - auc: 0.5969 - mean_squared_error: 0.2553 - val_loss: 0.7996 - val_auc: 0.6335 - val_mean_squared_error: 0.2456\n",
      "Epoch 4/500\n",
      "42/42 [==============================] - 12s 295ms/step - loss: 0.5849 - auc: 0.6274 - mean_squared_error: 0.2506 - val_loss: 0.7849 - val_auc: 0.6490 - val_mean_squared_error: 0.2436\n",
      "Epoch 5/500\n",
      "42/42 [==============================] - 12s 286ms/step - loss: 0.5764 - auc: 0.6444 - mean_squared_error: 0.2471 - val_loss: 0.5246 - val_auc: 0.6508 - val_mean_squared_error: 0.2416\n",
      "Epoch 6/500\n",
      "42/42 [==============================] - 12s 287ms/step - loss: 0.5645 - auc: 0.6639 - mean_squared_error: 0.2366 - val_loss: 0.7701 - val_auc: 0.6761 - val_mean_squared_error: 0.2328\n",
      "Epoch 7/500\n",
      "42/42 [==============================] - 12s 282ms/step - loss: 0.5638 - auc: 0.6724 - mean_squared_error: 0.2353 - val_loss: 0.5349 - val_auc: 0.6767 - val_mean_squared_error: 0.2320\n",
      "Epoch 8/500\n",
      "42/42 [==============================] - 12s 281ms/step - loss: 0.5595 - auc: 0.6840 - mean_squared_error: 0.2293 - val_loss: 0.5801 - val_auc: 0.6903 - val_mean_squared_error: 0.2263\n",
      "Epoch 9/500\n",
      "42/42 [==============================] - 12s 278ms/step - loss: 0.5563 - auc: 0.6924 - mean_squared_error: 0.2257 - val_loss: 0.5556 - val_auc: 0.6969 - val_mean_squared_error: 0.2233\n",
      "Epoch 10/500\n",
      "42/42 [==============================] - 12s 291ms/step - loss: 0.5544 - auc: 0.6997 - mean_squared_error: 0.2224 - val_loss: 0.5442 - val_auc: 0.7038 - val_mean_squared_error: 0.2203\n",
      "Epoch 11/500\n",
      "42/42 [==============================] - 12s 286ms/step - loss: 0.5450 - auc: 0.7067 - mean_squared_error: 0.2194 - val_loss: 0.5225 - val_auc: 0.7093 - val_mean_squared_error: 0.2177\n",
      "Epoch 12/500\n",
      "42/42 [==============================] - 12s 296ms/step - loss: 0.5513 - auc: 0.7130 - mean_squared_error: 0.2165 - val_loss: 0.5260 - val_auc: 0.7160 - val_mean_squared_error: 0.2149\n",
      "Epoch 13/500\n",
      "42/42 [==============================] - 12s 295ms/step - loss: 0.5460 - auc: 0.7187 - mean_squared_error: 0.2141 - val_loss: 0.4975 - val_auc: 0.7203 - val_mean_squared_error: 0.2129\n",
      "Epoch 14/500\n",
      "42/42 [==============================] - 12s 288ms/step - loss: 0.5465 - auc: 0.7239 - mean_squared_error: 0.2117 - val_loss: 0.4920 - val_auc: 0.7248 - val_mean_squared_error: 0.2108\n",
      "Epoch 15/500\n",
      "42/42 [==============================] - 12s 290ms/step - loss: 0.5414 - auc: 0.7286 - mean_squared_error: 0.2095 - val_loss: 0.5163 - val_auc: 0.7304 - val_mean_squared_error: 0.2085\n",
      "Epoch 16/500\n",
      "42/42 [==============================] - 12s 291ms/step - loss: 0.5432 - auc: 0.7324 - mean_squared_error: 0.2079 - val_loss: 0.5067 - val_auc: 0.7336 - val_mean_squared_error: 0.2071\n",
      "Epoch 17/500\n",
      "42/42 [==============================] - 12s 291ms/step - loss: 0.5408 - auc: 0.7361 - mean_squared_error: 0.2063 - val_loss: 0.4844 - val_auc: 0.7366 - val_mean_squared_error: 0.2057\n",
      "Epoch 18/500\n",
      "42/42 [==============================] - 12s 288ms/step - loss: 0.5377 - auc: 0.7398 - mean_squared_error: 0.2046 - val_loss: 0.4859 - val_auc: 0.7399 - val_mean_squared_error: 0.2042\n",
      "Epoch 19/500\n",
      "42/42 [==============================] - 12s 293ms/step - loss: 0.5370 - auc: 0.7428 - mean_squared_error: 0.2031 - val_loss: 0.4841 - val_auc: 0.7436 - val_mean_squared_error: 0.2026\n",
      "Epoch 20/500\n",
      "42/42 [==============================] - 12s 287ms/step - loss: 0.5369 - auc: 0.7459 - mean_squared_error: 0.2018 - val_loss: 0.4906 - val_auc: 0.7466 - val_mean_squared_error: 0.2012\n",
      "Epoch 21/500\n",
      "42/42 [==============================] - 12s 288ms/step - loss: 0.5344 - auc: 0.7486 - mean_squared_error: 0.2006 - val_loss: 0.5812 - val_auc: 0.7501 - val_mean_squared_error: 0.2000\n",
      "Epoch 22/500\n",
      "42/42 [==============================] - 12s 292ms/step - loss: 0.5355 - auc: 0.7495 - mean_squared_error: 0.2003 - val_loss: 0.4900 - val_auc: 0.7500 - val_mean_squared_error: 0.1999\n",
      "Epoch 23/500\n",
      "42/42 [==============================] - 12s 292ms/step - loss: 0.5323 - auc: 0.7518 - mean_squared_error: 0.1993 - val_loss: 0.4949 - val_auc: 0.7523 - val_mean_squared_error: 0.1990\n",
      "Epoch 24/500\n",
      "42/42 [==============================] - 12s 281ms/step - loss: 0.5348 - auc: 0.7539 - mean_squared_error: 0.1984 - val_loss: 0.5106 - val_auc: 0.7548 - val_mean_squared_error: 0.1980\n",
      "Epoch 25/500\n",
      "42/42 [==============================] - 12s 282ms/step - loss: 0.5274 - auc: 0.7558 - mean_squared_error: 0.1977 - val_loss: 0.4866 - val_auc: 0.7559 - val_mean_squared_error: 0.1974\n",
      "Epoch 26/500\n",
      "42/42 [==============================] - 12s 287ms/step - loss: 0.5346 - auc: 0.7576 - mean_squared_error: 0.1968 - val_loss: 0.5577 - val_auc: 0.7559 - val_mean_squared_error: 0.1975\n",
      "Epoch 27/500\n",
      "42/42 [==============================] - 12s 291ms/step - loss: 0.5306 - auc: 0.7583 - mean_squared_error: 0.1965 - val_loss: 0.5120 - val_auc: 0.7592 - val_mean_squared_error: 0.1961\n",
      "Epoch 28/500\n",
      "42/42 [==============================] - 12s 292ms/step - loss: 0.5270 - auc: 0.7599 - mean_squared_error: 0.1959 - val_loss: 0.4819 - val_auc: 0.7601 - val_mean_squared_error: 0.1957\n",
      "Epoch 29/500\n",
      "42/42 [==============================] - 12s 296ms/step - loss: 0.5294 - auc: 0.7615 - mean_squared_error: 0.1952 - val_loss: 0.4823 - val_auc: 0.7616 - val_mean_squared_error: 0.1950\n",
      "Epoch 30/500\n",
      "42/42 [==============================] - 12s 292ms/step - loss: 0.5265 - auc: 0.7630 - mean_squared_error: 0.1945 - val_loss: 0.4945 - val_auc: 0.7635 - val_mean_squared_error: 0.1942\n",
      "Epoch 31/500\n",
      "42/42 [==============================] - 12s 293ms/step - loss: 0.5231 - auc: 0.7644 - mean_squared_error: 0.1939 - val_loss: 0.5120 - val_auc: 0.7652 - val_mean_squared_error: 0.1936\n",
      "Epoch 32/500\n",
      "42/42 [==============================] - 12s 291ms/step - loss: 0.5272 - auc: 0.7656 - mean_squared_error: 0.1934 - val_loss: 0.5028 - val_auc: 0.7662 - val_mean_squared_error: 0.1932\n",
      "Epoch 33/500\n",
      "42/42 [==============================] - 13s 299ms/step - loss: 0.5241 - auc: 0.7669 - mean_squared_error: 0.1930 - val_loss: 0.4983 - val_auc: 0.7673 - val_mean_squared_error: 0.1927\n",
      "Epoch 34/500\n",
      "42/42 [==============================] - 12s 288ms/step - loss: 0.5253 - auc: 0.7680 - mean_squared_error: 0.1925 - val_loss: 0.4867 - val_auc: 0.7683 - val_mean_squared_error: 0.1923\n",
      "Epoch 35/500\n",
      "42/42 [==============================] - 12s 286ms/step - loss: 0.5220 - auc: 0.7693 - mean_squared_error: 0.1920 - val_loss: 0.4994 - val_auc: 0.7688 - val_mean_squared_error: 0.1921\n",
      "Epoch 36/500\n",
      "42/42 [==============================] - 12s 292ms/step - loss: 0.5252 - auc: 0.7702 - mean_squared_error: 0.1915 - val_loss: 0.5109 - val_auc: 0.7706 - val_mean_squared_error: 0.1913\n",
      "Epoch 37/500\n",
      "42/42 [==============================] - 12s 291ms/step - loss: 0.5199 - auc: 0.7711 - mean_squared_error: 0.1912 - val_loss: 0.4848 - val_auc: 0.7712 - val_mean_squared_error: 0.1911\n",
      "Epoch 38/500\n",
      "42/42 [==============================] - 12s 292ms/step - loss: 0.5224 - auc: 0.7721 - mean_squared_error: 0.1907 - val_loss: 0.5094 - val_auc: 0.7725 - val_mean_squared_error: 0.1905\n",
      "Epoch 39/500\n",
      "42/42 [==============================] - 12s 284ms/step - loss: 0.5224 - auc: 0.7729 - mean_squared_error: 0.1904 - val_loss: 0.5507 - val_auc: 0.7718 - val_mean_squared_error: 0.1909\n",
      "Epoch 40/500\n",
      "42/42 [==============================] - 12s 282ms/step - loss: 0.5180 - auc: 0.7733 - mean_squared_error: 0.1903 - val_loss: 0.5091 - val_auc: 0.7737 - val_mean_squared_error: 0.1901\n",
      "Epoch 41/500\n",
      "42/42 [==============================] - 12s 289ms/step - loss: 0.5179 - auc: 0.7741 - mean_squared_error: 0.1899 - val_loss: 0.4916 - val_auc: 0.7738 - val_mean_squared_error: 0.1900\n",
      "Epoch 42/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 12s 283ms/step - loss: 0.5139 - auc: 0.7750 - mean_squared_error: 0.1895 - val_loss: 0.4984 - val_auc: 0.7754 - val_mean_squared_error: 0.1894\n",
      "Epoch 43/500\n",
      "42/42 [==============================] - 12s 291ms/step - loss: 0.5170 - auc: 0.7758 - mean_squared_error: 0.1892 - val_loss: 0.4906 - val_auc: 0.7756 - val_mean_squared_error: 0.1893\n",
      "Epoch 44/500\n",
      "42/42 [==============================] - 12s 293ms/step - loss: 0.5167 - auc: 0.7766 - mean_squared_error: 0.1888 - val_loss: 0.5307 - val_auc: 0.7771 - val_mean_squared_error: 0.1887\n",
      "Epoch 45/500\n",
      "42/42 [==============================] - 12s 284ms/step - loss: 0.5123 - auc: 0.7771 - mean_squared_error: 0.1887 - val_loss: 0.4994 - val_auc: 0.7774 - val_mean_squared_error: 0.1885\n",
      "Epoch 46/500\n",
      "42/42 [==============================] - 12s 296ms/step - loss: 0.5161 - auc: 0.7779 - mean_squared_error: 0.1884 - val_loss: 0.5355 - val_auc: 0.7783 - val_mean_squared_error: 0.1882\n",
      "Epoch 47/500\n",
      "42/42 [==============================] - 12s 284ms/step - loss: 0.5141 - auc: 0.7784 - mean_squared_error: 0.1882 - val_loss: 0.5033 - val_auc: 0.7786 - val_mean_squared_error: 0.1881\n",
      "Epoch 48/500\n",
      "42/42 [==============================] - 12s 292ms/step - loss: 0.5149 - auc: 0.7790 - mean_squared_error: 0.1879 - val_loss: 0.4896 - val_auc: 0.7792 - val_mean_squared_error: 0.1878\n",
      "Epoch 49/500\n",
      "42/42 [==============================] - 12s 274ms/step - loss: 0.5120 - auc: 0.7798 - mean_squared_error: 0.1876 - val_loss: 0.5545 - val_auc: 0.7803 - val_mean_squared_error: 0.1875\n",
      "Epoch 50/500\n",
      "42/42 [==============================] - 11s 260ms/step - loss: 0.5082 - auc: 0.7801 - mean_squared_error: 0.1876 - val_loss: 0.4948 - val_auc: 0.7802 - val_mean_squared_error: 0.1875\n",
      "Epoch 51/500\n",
      "42/42 [==============================] - 11s 265ms/step - loss: 0.5116 - auc: 0.7808 - mean_squared_error: 0.1873 - val_loss: 0.5508 - val_auc: 0.7812 - val_mean_squared_error: 0.1871\n",
      "Epoch 52/500\n",
      "42/42 [==============================] - 11s 267ms/step - loss: 0.5120 - auc: 0.7811 - mean_squared_error: 0.1872 - val_loss: 0.4939 - val_auc: 0.7812 - val_mean_squared_error: 0.1871\n",
      "Epoch 53/500\n",
      "42/42 [==============================] - 11s 264ms/step - loss: 0.5088 - auc: 0.7817 - mean_squared_error: 0.1869 - val_loss: 0.5404 - val_auc: 0.7821 - val_mean_squared_error: 0.1868\n",
      "Epoch 54/500\n",
      "42/42 [==============================] - 11s 257ms/step - loss: 0.5107 - auc: 0.7821 - mean_squared_error: 0.1868 - val_loss: 0.5363 - val_auc: 0.7824 - val_mean_squared_error: 0.1867\n",
      "Epoch 55/500\n",
      "42/42 [==============================] - 11s 257ms/step - loss: 0.5089 - auc: 0.7824 - mean_squared_error: 0.1867 - val_loss: 0.5177 - val_auc: 0.7827 - val_mean_squared_error: 0.1866\n",
      "Epoch 56/500\n",
      "42/42 [==============================] - 11s 267ms/step - loss: 0.5091 - auc: 0.7829 - mean_squared_error: 0.1865 - val_loss: 0.5393 - val_auc: 0.7833 - val_mean_squared_error: 0.1864\n",
      "Epoch 57/500\n",
      "42/42 [==============================] - 11s 272ms/step - loss: 0.5090 - auc: 0.7833 - mean_squared_error: 0.1864 - val_loss: 0.5016 - val_auc: 0.7834 - val_mean_squared_error: 0.1863\n",
      "Epoch 58/500\n",
      "42/42 [==============================] - 11s 271ms/step - loss: 0.5086 - auc: 0.7838 - mean_squared_error: 0.1862 - val_loss: 0.5734 - val_auc: 0.7842 - val_mean_squared_error: 0.1861\n",
      "Epoch 59/500\n",
      "42/42 [==============================] - 11s 270ms/step - loss: 0.5128 - auc: 0.7839 - mean_squared_error: 0.1862 - val_loss: 0.5049 - val_auc: 0.7840 - val_mean_squared_error: 0.1861\n",
      "Epoch 60/500\n",
      "42/42 [==============================] - 12s 275ms/step - loss: 0.5055 - auc: 0.7844 - mean_squared_error: 0.1860 - val_loss: 0.5099 - val_auc: 0.7845 - val_mean_squared_error: 0.1859\n",
      "Epoch 61/500\n",
      "42/42 [==============================] - 11s 271ms/step - loss: 0.5076 - auc: 0.7848 - mean_squared_error: 0.1858 - val_loss: 0.5559 - val_auc: 0.7851 - val_mean_squared_error: 0.1857\n",
      "Epoch 62/500\n",
      "42/42 [==============================] - 12s 276ms/step - loss: 0.5049 - auc: 0.7850 - mean_squared_error: 0.1858 - val_loss: 0.5095 - val_auc: 0.7852 - val_mean_squared_error: 0.1857\n",
      "Epoch 63/500\n",
      "42/42 [==============================] - 11s 272ms/step - loss: 0.5068 - auc: 0.7854 - mean_squared_error: 0.1856 - val_loss: 0.5149 - val_auc: 0.7857 - val_mean_squared_error: 0.1855\n",
      "Epoch 64/500\n",
      "42/42 [==============================] - 11s 266ms/step - loss: 0.5059 - auc: 0.7859 - mean_squared_error: 0.1854 - val_loss: 0.5292 - val_auc: 0.7861 - val_mean_squared_error: 0.1853\n",
      "Epoch 65/500\n",
      "42/42 [==============================] - 12s 274ms/step - loss: 0.5061 - auc: 0.7862 - mean_squared_error: 0.1853 - val_loss: 0.5111 - val_auc: 0.7863 - val_mean_squared_error: 0.1853\n",
      "Epoch 66/500\n",
      "42/42 [==============================] - 12s 284ms/step - loss: 0.5030 - auc: 0.7865 - mean_squared_error: 0.1852 - val_loss: 0.5114 - val_auc: 0.7867 - val_mean_squared_error: 0.1851\n",
      "Epoch 67/500\n",
      "42/42 [==============================] - 12s 279ms/step - loss: 0.5042 - auc: 0.7870 - mean_squared_error: 0.1850 - val_loss: 0.5127 - val_auc: 0.7868 - val_mean_squared_error: 0.1850\n",
      "Epoch 68/500\n",
      "42/42 [==============================] - 12s 285ms/step - loss: 0.5033 - auc: 0.7873 - mean_squared_error: 0.1849 - val_loss: 0.5651 - val_auc: 0.7876 - val_mean_squared_error: 0.1847\n",
      "Epoch 69/500\n",
      "42/42 [==============================] - 12s 281ms/step - loss: 0.5009 - auc: 0.7874 - mean_squared_error: 0.1848 - val_loss: 0.5113 - val_auc: 0.7876 - val_mean_squared_error: 0.1848\n",
      "Epoch 70/500\n",
      "42/42 [==============================] - 12s 287ms/step - loss: 0.5030 - auc: 0.7878 - mean_squared_error: 0.1847 - val_loss: 0.4976 - val_auc: 0.7878 - val_mean_squared_error: 0.1846\n",
      "Epoch 71/500\n",
      "42/42 [==============================] - 12s 284ms/step - loss: 0.5027 - auc: 0.7882 - mean_squared_error: 0.1845 - val_loss: 0.5135 - val_auc: 0.7883 - val_mean_squared_error: 0.1844\n",
      "Epoch 72/500\n",
      "42/42 [==============================] - 12s 280ms/step - loss: 0.5008 - auc: 0.7885 - mean_squared_error: 0.1844 - val_loss: 0.5435 - val_auc: 0.7888 - val_mean_squared_error: 0.1843\n",
      "Epoch 73/500\n",
      "42/42 [==============================] - 12s 282ms/step - loss: 0.4995 - auc: 0.7888 - mean_squared_error: 0.1843 - val_loss: 0.5202 - val_auc: 0.7889 - val_mean_squared_error: 0.1842\n",
      "Epoch 74/500\n",
      "42/42 [==============================] - 12s 285ms/step - loss: 0.5020 - auc: 0.7891 - mean_squared_error: 0.1842 - val_loss: 0.5094 - val_auc: 0.7892 - val_mean_squared_error: 0.1841\n",
      "Epoch 75/500\n",
      "42/42 [==============================] - 12s 279ms/step - loss: 0.4997 - auc: 0.7894 - mean_squared_error: 0.1841 - val_loss: 0.6280 - val_auc: 0.7897 - val_mean_squared_error: 0.1840\n",
      "Epoch 76/500\n",
      "42/42 [==============================] - 12s 285ms/step - loss: 0.4975 - auc: 0.7893 - mean_squared_error: 0.1842 - val_loss: 0.5223 - val_auc: 0.7895 - val_mean_squared_error: 0.1841\n",
      "Epoch 77/500\n",
      "27/42 [==================>...........] - ETA: 3s - loss: 0.4985 - auc: 0.7896 - mean_squared_error: 0.1841"
     ]
    }
   ],
   "source": [
    "model_prefix = \"testing_ckt\"\n",
    "\n",
    "batch_size = 512\n",
    "selected_partitions = [0,1,2,3] # 5'th partition is completely left out.\n",
    "\n",
    "\n",
    "model_id = 0\n",
    "for train_partitions_indices, test_partition_indices in cv_split( selected_partitions, 1 ):\n",
    "    if 3 in test_partition_indices:\n",
    "        continue # Limit training of only 3 models\n",
    "    \n",
    "    model = get_model()\n",
    "    \n",
    "    print \"Training model %d on training partitions %s and test partitions %s\" % (model_id, str(train_partitions_indices), str(test_partition_indices))\n",
    "\n",
    "    # Prepare training partitions\n",
    "    partitions_pos_train = partition_soluble[ partition_soluble.Partition.isin( train_partitions_indices ) ]\n",
    "    partitions_neg_train = partition_insoluble[ partition_insoluble.Partition.isin( train_partitions_indices ) ]\n",
    "\n",
    "    # Prepare earlystoping data\n",
    "    partitions_pos_test = partition_soluble[ partition_soluble.Partition.isin( test_partition_indices ) ]\n",
    "    partitions_neg_test = partition_insoluble[ partition_insoluble.Partition.isin( test_partition_indices ) ]\n",
    "\n",
    "    sequences_int_test = sequences_int.loc[list(partitions_pos_test.ProteinID) + list(partitions_neg_test.ProteinID)]\n",
    "    target_values_test = target_values.loc[list(partitions_pos_test.ProteinID) + list(partitions_neg_test.ProteinID)]\n",
    "\n",
    "    # Use positive data to estimate epoch size\n",
    "    epoch_size = calc_epoch_size(partitions_pos_train.Weight)\n",
    "\n",
    "    print \"   Training data: %d\" % (len(partitions_pos_train) + len(partitions_neg_train))\n",
    "    print \"   Testing data: %d\" % len(sequences_int_test)\n",
    "    \n",
    "    \n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_auc',\n",
    "                                   mode='max',\n",
    "                                   patience=50, \n",
    "                                   min_delta=0.0001,\n",
    "                                   verbose=1)\n",
    "\n",
    "    model_checkpoint = ModelCheckpoint('models/%s_%d.h5' % (model_prefix, model_id),\n",
    "                                       monitor='val_auc',\n",
    "                                       mode='max',\n",
    "                                       verbose=0,\n",
    "                                       save_best_only=True,\n",
    "                                       save_weights_only=True)\n",
    "\n",
    "    reduce_lr_on_plateau = ReduceLROnPlateau(monitor='val_auc',\n",
    "                                             mode='max',\n",
    "                                             factor=0.5,\n",
    "                                             patience=8,\n",
    "                                             verbose=1,\n",
    "                                             cooldown=10,\n",
    "                                             min_lr=0.00001)\n",
    "\n",
    "    tensorboard = TensorBoard(log_dir=\"./graph/%s_%d\" % (model_prefix, model_id), \n",
    "                              write_images=True,\n",
    "                              histogram_freq=0)\n",
    "\n",
    "    validation_data = (sequences_int_test, target_values_test)\n",
    "\n",
    "    model.fit_generator( generate_batch(partitions_pos_train, \n",
    "                                        partitions_neg_train,\n",
    "                                        batch_size=batch_size,\n",
    "                                        target_values=target_values,\n",
    "                                        sequences=sequences,\n",
    "                                        blosum_matrix=None,\n",
    "                                        padding_length=1000),\n",
    "                         steps_per_epoch = epoch_size/(batch_size/2),\n",
    "                         epochs = 500,\n",
    "                         callbacks=[early_stopping, model_checkpoint, reduce_lr_on_plateau, tensorboard],\n",
    "                         validation_data=validation_data,\n",
    "                         verbose = 1)\n",
    "\n",
    "    model_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "y_pred = model.predict(validation_data[0], verbose=1)\n",
    "\n",
    "print roc_auc_score(validation_data[1], y_pred.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at history output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = histories[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "#['acc', 'loss', 'val_acc', 'val_loss']\n",
    "x = range(len(history.history.values()[0]))\n",
    "ax = fig.add_subplot(111)\n",
    "line, = ax.plot(x, history.history[\"auc\"], color='b', linewidth=\"1\")\n",
    "line, = ax.plot(x, history.history[\"val_auc\"], color='r', linewidth=\"1\")\n",
    "ax.set_title(\"%d\" % (i+1))\n",
    "lgd = ax.legend([\"AUC (train)\", \"AUC (stop)\"], loc=\"upper left\", bbox_to_anchor=(1,0.905))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "#['acc', 'loss', 'val_acc', 'val_loss']\n",
    "x = range(len(history.history.values()[0]))\n",
    "ax = fig.add_subplot(111)\n",
    "line, = ax.plot(x, history.history[\"loss\"], color='b', linewidth=\"1\")\n",
    "line, = ax.plot(x, history.history[\"val_loss\"], color='r', linewidth=\"1\")\n",
    "ax.set_title(\"%d\" % (i+1))\n",
    "lgd = ax.legend([\"Log-loss (train)\", \"Log-loss (stop)\"], loc=\"upper left\", bbox_to_anchor=(1,0.905))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
